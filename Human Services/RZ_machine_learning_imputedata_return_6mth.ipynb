{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sqlalchemy import create_engine\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up sqlalchemy engine\n",
    "engine = create_engine('postgresql://10.10.2.10/appliedda')\n",
    "\n",
    "# See all available schemas:\n",
    "pd.read_sql(\"SELECT schema_name FROM information_schema.schemata LIMIT 10;\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set \n",
    "#select_string = \"SELECT recptno,return_1yr, foodstamp,tanf,grantf,age,spell_cancel \"\n",
    "select_string = \"SELECT * \"\n",
    "select_string += \" FROM c6.partial_evaluate\"\n",
    "select_string += \" WHERE (oldSpell_end>='2010-05-31' AND oldSpell_end<='2010-12-31')\"\n",
    "select_string += \"  \"\n",
    "\n",
    "print(select_string)\n",
    "\n",
    "##give pd dataframe a name\n",
    "df_training = pd.read_sql(select_string, engine)\n",
    "\n",
    "print(\"Number of rows returned: \" + str(len(df_training)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing set \n",
    "#select_string = \"SELECT recptno,return_1yr, foodstamp,tanf,grantf,age,spell_cancel\"\n",
    "select_string = \"SELECT * \"\n",
    "select_string += \" FROM c6.partial_evaluate\"\n",
    "select_string += \" WHERE (oldSpell_end>='2011-01-01' AND oldSpell_end<='2011-12-31')\"\n",
    "select_string += \"  \"\n",
    "\n",
    "print(select_string)\n",
    "\n",
    "##give pd dataframe a name\n",
    "df_testing = pd.read_sql(select_string, engine)\n",
    "\n",
    "print(\"Number of rows returned: \" + str(len(df_testing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isnan_training_rows = df_training.isnull().any(axis=1) # Find the rows where there are NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[isnan_training_rows].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isnan_training_columns = df_training.isnull().any(axis=0) # Find the rows where there are NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_training_columns[isnan_training_columns==True].item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many null value is there?\n",
    "df_training['age'].isnull().sum()\n",
    "#trainset['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def normal\n",
    "trainset['wage_sum_tm4']/trainset['wage_sum_tm4'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training['age']=df_training['age'].fillna(df_training['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of variables that need to be turn to dummies: 'quarter_t', 'edlevel','workexp','district',\n",
    "#'race','sex','rootrace','foreignbn'\n",
    "#quarter_t\n",
    "df_training['t_q1']=(df_training['quarter_t']==1)\n",
    "df_training['t_q2']=(df_training['quarter_t']==2)\n",
    "df_training['t_q3']=(df_training['quarter_t']==3)\n",
    "#edlevel\n",
    "df_training['edu_hs'] = (df_training['edlevel']==1)\n",
    "df_training['edu_hsgrad'] = (df_training['edlevel']==2)\n",
    "df_training['edu_coll'] = (df_training['edlevel']==3)\n",
    "df_training['edu_collgrad'] = (df_training['edlevel']==4)\n",
    "#workexp\n",
    "df_training['work_prof'] = (df_training['workexp'] == 2)\n",
    "df_training['work_othermgr'] = (df_training['workexp'] == 3)\n",
    "df_training['work_clerical'] = (df_training['workexp'] == 4)\n",
    "df_training['work_sales'] = (df_training['workexp'] == 5)\n",
    "df_training['work_crafts'] = (df_training['workexp'] == 6)\n",
    "df_training['work_oper'] = (df_training['workexp'] == 7)\n",
    "df_training['work_service'] = (df_training['workexp'] == 8)\n",
    "df_training['work_labor'] = (df_training['workexp'] == 9)\n",
    "#district\n",
    "df_training['dist_cookcty'] = (df_training['district'] ==1)\n",
    "df_training['dist_downstate'] = (df_training['district'] ==0)\n",
    "#race from assistance case: 1,2,3\n",
    "df_training['race_1'] = (df_training['race'] == 1)\n",
    "df_training['race_2'] = (df_training['race'] == 2)\n",
    "#sex\n",
    "df_training['male'] = (df_training['sex'] == 1)\n",
    "df_training['female'] = (df_training['sex'] == 2)\n",
    "#rootrace\n",
    "df_training['hh_white'] = (df_training['rootrace'] == 1)\n",
    "df_training['hh_black'] = (df_training['rootrace'] == 2)\n",
    "df_training['hh_native'] = (df_training['rootrace'] == 3)\n",
    "df_training['hh_hispanic'] = (df_training['rootrace'] == 6)\n",
    "df_training['hh_asian'] = (df_training['rootrace'] == 7)\n",
    "#foreignbn: 0,1,2,3,4,5\n",
    "df_training['foreignbn_1'] = (df_training['foreignbn'] == 1)\n",
    "df_training['foreignbn_2'] = (df_training['foreignbn'] == 2)\n",
    "df_training['foreignbn_3'] = (df_training['foreignbn'] == 3)\n",
    "df_training['foreignbn_4'] = (df_training['foreignbn'] == 4)\n",
    "df_training['foreignbn_5'] = (df_training['foreignbn'] == 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dummies\n",
    "#df_training_dum=pd.get_dummies(df_training,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[df_training['spell_cancel']==0]['return_6mth'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[df_training['spell_cancel']==1]['return_6mth'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_training['spell_cancel'],df_training['return_6mth'],normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate testset\n",
    "isnan_training_rows = df_testing.isnull().any(axis=1) # Find the rows where there are \n",
    "df_testing[isnan_training_rows].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing['age'].isnull().sum()\n",
    "#trainset['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of variables that need to be turn to dummies: 'quarter_t', 'edlevel','workexp','district',\n",
    "#'race','sex','rootrace','foreignbn'\n",
    "#quarter_t\n",
    "df_testing['t_q1']=(df_testing['quarter_t']==1)\n",
    "df_testing['t_q2']=(df_testing['quarter_t']==2)\n",
    "df_testing['t_q3']=(df_testing['quarter_t']==3)\n",
    "#edlevel\n",
    "df_testing['edu_hs'] = (df_testing['edlevel']==1)\n",
    "df_testing['edu_hsgrad'] = (df_testing['edlevel']==2)\n",
    "df_testing['edu_coll'] = (df_testing['edlevel']==3)\n",
    "df_testing['edu_collgrad'] = (df_testing['edlevel']==4)\n",
    "#workexp\n",
    "df_testing['work_prof'] = (df_testing['workexp'] == 2)\n",
    "df_testing['work_othermgr'] = (df_testing['workexp'] == 3)\n",
    "df_testing['work_clerical'] = (df_testing['workexp'] == 4)\n",
    "df_testing['work_sales'] = (df_testing['workexp'] == 5)\n",
    "df_testing['work_crafts'] = (df_testing['workexp'] == 6)\n",
    "df_testing['work_oper'] = (df_testing['workexp'] == 7)\n",
    "df_testing['work_service'] = (df_testing['workexp'] == 8)\n",
    "df_testing['work_labor'] = (df_testing['workexp'] == 9)\n",
    "#district\n",
    "df_testing['dist_cookcty'] = (df_testing['district'] ==1)\n",
    "df_testing['dist_downstate'] = (df_testing['district'] ==0)\n",
    "#race from assistance case: 1,2,3\n",
    "df_testing['race_1'] = (df_testing['race'] == 1)\n",
    "df_testing['race_2'] = (df_testing['race'] == 2)\n",
    "#sex\n",
    "df_testing['male'] = (df_testing['sex'] == 1)\n",
    "df_testing['female'] = (df_testing['sex'] == 2)\n",
    "#rootrace\n",
    "df_testing['hh_white'] = (df_testing['rootrace'] == 1)\n",
    "df_testing['hh_black'] = (df_testing['rootrace'] == 2)\n",
    "df_testing['hh_native'] = (df_testing['rootrace'] == 3)\n",
    "df_testing['hh_hispanic'] = (df_testing['rootrace'] == 6)\n",
    "df_testing['hh_asian'] = (df_testing['rootrace'] == 7)\n",
    "#foreignbn: 0,1,2,3,4,5\n",
    "df_testing['foreignbn_1'] = (df_testing['foreignbn'] == 1)\n",
    "df_testing['foreignbn_2'] = (df_testing['foreignbn'] == 2)\n",
    "df_testing['foreignbn_3'] = (df_testing['foreignbn'] == 3)\n",
    "df_testing['foreignbn_4'] = (df_testing['foreignbn'] == 4)\n",
    "df_testing['foreignbn_5'] = (df_testing['foreignbn'] == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label and featre global\n",
    "sel_label='return_6mth'\n",
    "#'never_return','return_3mth','return_6mth','return_1yr','return_1yr6mth','return_2yr',\n",
    "sel_features=['age','foodstamp','tanf','spell_cancel','num_emp_tp4','wage_sum_tp4','wage_high_tp4','num_emp_tp3',\n",
    "             'wage_sum_tp3','wage_high_tp3','num_emp_tp2','wage_sum_tp2','wage_high_tp2','num_emp_tp1','wage_sum_tp1',\n",
    "             'wage_high_tp1','num_emp_tm1','wage_sum_tm1','wage_high_tm1','num_emp_tm2','wage_sum_tm2',\n",
    "             'wage_high_tm2','num_emp_tm3','wage_sum_tm3','wage_high_tm3','num_emp_tm4','wage_sum_tm4',\n",
    "             'wage_high_tm4','wage_sum_tp1t4','wage_sum_tm1t4','spell_length','n_prespells','max_spell_length',\n",
    "'min_spell_length','avg_spell_length','total_foodstamp_utlnow','total_tanf_utlnow','marstat','homeless','hh_counts',\n",
    "'t_q1','t_q2','t_q3','edu_hs','edu_hsgrad','edu_coll','edu_collgrad','work_prof','work_othermgr','work_clerical',\n",
    "'work_sales','work_crafts','work_oper','work_service','work_labor','dist_cookcty','dist_downstate','race_1',\n",
    "'race_2','male','female','hh_white','hh_black','hh_native','hh_hispanic','hh_asian','foreignbn_1','foreignbn_2',\n",
    "'foreignbn_3','foreignbn_4','foreignbn_5']\n",
    "#sel_featre_base=['foodstamp', 'tanf', 'grantf', 'age']\n",
    "#'foodstamp',tanf','grantf','age']\n",
    "#sel_featre_pls=['foodstamp','tanf','grantf','age','spell_cancel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use conventions typically used in python scikitlearn\n",
    "y_train=df_training[sel_label].values\n",
    "X_train=df_training[sel_features].values\n",
    "#X_train_base=trainset[sel_feature_base].values\n",
    "#X_train_plus=trainset[sel_feature_plus].values\n",
    "\n",
    "y_test=df_testing[sel_label].values\n",
    "X_test=df_testing[sel_features].values\n",
    "#X_test_base=testset[sel_feature_base].values\n",
    "#X_test_plus=testset[sel_feature_plus].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler=Normalizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_X_train=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean of un-normalized features for X_train:{}\".format(np.mean(X_train)))\n",
    "print (\"Mean of un-normalized features for X_test:{}\".format(np.mean(X_test)))\n",
    "print (\"Mean of normalized features for X_train:{}\".format(np.mean(normalized_X_train)))\n",
    "print (\"Mean of normalized features for X_test:{}\".format(np.mean(normalized_X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"std of un-normalized features for X_train:{}\".format(np.std(X_train)))\n",
    "print (\"std of un-normalized features for X_test:{}\".format(np.std(X_test)))\n",
    "print (\"std of normalized features for X_train:{}\".format(np.std(normalized_X_train)))\n",
    "print (\"std of normalized features for X_test:{}\".format(np.std(normalized_X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=normalized_X_train\n",
    "X_test=normalized_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_training.shape[0]))\n",
    "df_training['return_6mth'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_testing.shape[0]))\n",
    "df_testing['return_6mth'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit a model-logit\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(penalty='l1', C=1e5)\n",
    "model.fit( X_train, y_train )\n",
    "#model.fit( X_train_base, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###model understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"The coefficients for each of the features are \" \n",
    "zip(sel_features, model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_coef = np.std(X_test,0)*model.coef_\n",
    "zip(sel_features, std_coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  from our \"predictors\" using the model.\n",
    "y_scores = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_scores, kde=True,hist=True, rug=False)\n",
    "#hist=false would be the final option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing['y_score'] = y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing[['recptno', 'y_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calc_threshold = lambda x,y: 0 if x < y else 1 \n",
    "predicted = np.array( [calc_threshold(score,0.5) for score in y_scores] )\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The count of true negatives is conf_matrix[0,0], \n",
    "print(\"true negative\" ) \n",
    "print conf_matrix[0,0]\n",
    "#false negatives conf_matrix[1,0], \n",
    "print (\"false negative\")\n",
    "print conf_matrix[1,0]\n",
    "#true positives conf_matrix[1,1],\n",
    "print (\"true positive\")\n",
    "print conf_matrix[1,1]\n",
    "#and false_positives conf_matrix[0,1].\n",
    "print (\"false positive\")\n",
    "print conf_matrix[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an accuracy score by comparing expected to predicted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional metrics that are often used are **precision** and **recall**. \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds we can vary and tune the precision and recall of a given classifier. A conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###precision and recall at k%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###survey of algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "       'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, random_state=17, n_estimators=10),\n",
    "        'NB': GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_clfs = ['RF', 'ET', 'LR', 'SGD', 'GB', 'NB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_p_at_k = 0\n",
    "for clfNM in sel_clfs:\n",
    "    clf = clfs[clfNM]\n",
    "    clf.fit( X_train, y_train )\n",
    "    print clf\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    predicted = np.array(y_score)\n",
    "    expected = np.array(y_test)\n",
    "    plot_precision_recall_n(expected,predicted, clfNM)\n",
    "    p_at_1 = precision_at_k(expected,y_score, 0.01)\n",
    "    if max_p_at_k < p_at_1:\n",
    "        max_p_at_k = p_at_1\n",
    "    print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###access model against baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_p_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_score = [random.uniform(0,1) for i in enumerate(y_test)] \n",
    "random_predicted = np.array( [calc_threshold(score,0.5) for score in random_score] )\n",
    "random_p_at_5 = precision_at_k(expected,random_predicted, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reenter_predicted = np.array([ 1 if foodstamp > 0 else 0 for foodstamp in df_testing.foodstamp.values])\n",
    "reenter_p_at_1 = precision_at_k(expected,reenter_predicted,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_non_reenter = np.array([0 if spell_cancel > 0 else 1 for spell_cancel in df_testing.spell_cancel.values])\n",
    "all_non_reenter_p_at_1 = precision_at_k(expected, all_non_reenter,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=2.25, rc={\"lines.linewidth\":2.25, \"lines.markersize\":8})\n",
    "fig, ax = plt.subplots(1, figsize=(22,12))\n",
    "sns.barplot(['Random','all foodstamp', 'spell cancel','Model'],\n",
    "            [random_p_at_5,reenter_p_at_1, all_non_reenter_p_at_1,  max_p_at_k],\n",
    "            palette=['#6F777D','#6F777D','#6F777D','#800000'])\n",
    "sns.despine()\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('precision at 1%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
