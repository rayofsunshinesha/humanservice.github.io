{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing,cross_validation,svm,metrics,tree,decomposition\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier,Perceptron,OrthogonalMatchingPursuit,RandomizedLogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up sqlalchemy engine\n",
    "engine = create_engine('postgresql://10.10.2.10/appliedda')\n",
    "\n",
    "# See all available schemas:\n",
    "pd.read_sql(\"SELECT schema_name FROM information_schema.schemata LIMIT 10;\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can look at column names within tables:\n",
    "df=pd.read_sql(\"SELECT * FROM c6.partial_evaluate WHERE (oldSpell_end>='2010-05-31' AND oldSpell_end<='2012-12-31');\",engine)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_training_rows = df.isnull().any(axis=1) # Find the rows where there are NaNs\n",
    "df[isnan_training_rows].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_training_columns = df.isnull().any(axis=0) # Find the columns where there are NaNs\n",
    "isnan_training_columns[isnan_training_columns==True].item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of variables that need to be turn to dummies: 'quarter_t', 'edlevel','workexp','district',\n",
    "#'race','sex','rootrace','foreignbn'\n",
    "#quarter_t\n",
    "df['t_q1']=(df['quarter_t']==1)\n",
    "df['t_q2']=(df['quarter_t']==2)\n",
    "df['t_q3']=(df['quarter_t']==3)\n",
    "#edlevel\n",
    "df['edu_hs'] = (df['edlevel']==1)\n",
    "df['edu_hsgrad'] = (df['edlevel']==2)\n",
    "df['edu_coll'] = (df['edlevel']==3)\n",
    "df['edu_collgrad'] = (df['edlevel']==4)\n",
    "#workexp\n",
    "df['work_prof'] = (df['workexp'] == 2)\n",
    "df['work_othermgr'] = (df['workexp'] == 3)\n",
    "df['work_clerical'] = (df['workexp'] == 4)\n",
    "df['work_sales'] = (df['workexp'] == 5)\n",
    "df['work_crafts'] = (df['workexp'] == 6)\n",
    "df['work_oper'] = (df['workexp'] == 7)\n",
    "df['work_service'] = (df['workexp'] == 8)\n",
    "df['work_labor'] = (df['workexp'] == 9)\n",
    "#district\n",
    "df['dist_cookcty'] = (df['district'] ==1)\n",
    "df['dist_downstate'] = (df['district'] ==0)\n",
    "#race from assistance case: 1,2,3\n",
    "df['race_1'] = (df['race'] == 1)\n",
    "df['race_2'] = (df['race'] == 2)\n",
    "#sex\n",
    "df['male'] = (df['sex'] == 1)\n",
    "df['female'] = (df['sex'] == 2)\n",
    "#rootrace\n",
    "df['hh_white'] = (df['rootrace'] == 1)\n",
    "df['hh_black'] = (df['rootrace'] == 2)\n",
    "df['hh_native'] = (df['rootrace'] == 3)\n",
    "df['hh_hispanic'] = (df['rootrace'] == 6)\n",
    "df['hh_asian'] = (df['rootrace'] == 7)\n",
    "#foreignbn: 0,1,2,3,4,5\n",
    "df['foreignbn_1'] = (df['foreignbn'] == 1)\n",
    "df['foreignbn_2'] = (df['foreignbn'] == 2)\n",
    "df['foreignbn_3'] = (df['foreignbn'] == 3)\n",
    "df['foreignbn_4'] = (df['foreignbn'] == 4)\n",
    "df['foreignbn_5'] = (df['foreignbn'] == 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df[(df['oldspell_end'] >=datetime.date(2010,5,31)) & (df['oldspell_end'] <=datetime.date(2010,12,31))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "trainset['age']=trainset['age'].fillna(trainset['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deleted due to disclosure control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainset[trainset['spell_cancel']==1]['return_1yr'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testset.crosstab['return_1yr','spell_cancel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###mask the data and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label and feature global\n",
    "sel_label='return_1yr'\n",
    "sel_features=['foodstamp','tanf','spell_cancel','num_emp_tp4','wage_sum_tp4','wage_high_tp4','num_emp_tp3',\n",
    "             'wage_sum_tp3','wage_high_tp3','num_emp_tp2','wage_sum_tp2','wage_high_tp2','num_emp_tp1','wage_sum_tp1',\n",
    "             'wage_high_tp1','num_emp_tm1','wage_sum_tm1','wage_high_tm1','num_emp_tm2','wage_sum_tm2',\n",
    "             'wage_high_tm2','num_emp_tm3','wage_sum_tm3','wage_high_tm3','num_emp_tm4','wage_sum_tm4',\n",
    "             'wage_high_tm4','wage_sum_tp1t4','wage_sum_tm1t4','spell_length','n_prespells','max_spell_length',\n",
    "'min_spell_length','avg_spell_length','total_foodstamp_utlnow','total_tanf_utlnow','marstat','homeless','hh_counts',\n",
    "'t_q1','t_q2','t_q3','edu_hs','edu_hsgrad','edu_coll','edu_collgrad','work_prof','work_othermgr','work_clerical',\n",
    "'work_sales','work_crafts','work_oper','work_service','work_labor','dist_cookcty','dist_downstate','race_1',\n",
    "'race_2','male','female','hh_white','hh_black','hh_native','hh_hispanic','hh_asian','foreignbn_1','foreignbn_2',\n",
    "'foreignbn_3','foreignbn_4','foreignbn_5']\n",
    "sel_features_spell=['foodstamp','tanf','spell_cancel','spell_length','n_prespells','max_spell_length',\n",
    "                    'min_spell_length','avg_spell_length','total_foodstamp_utlnow','total_tanf_utlnow']\n",
    "sel_features_wage=['num_emp_tm1','wage_sum_tm1','wage_high_tm1',\n",
    "              'num_emp_tm2','wage_sum_tm2','wage_high_tm2',\n",
    "             'num_emp_tm3','wage_sum_tm3','wage_high_tm3',\n",
    "             'num_emp_tm4','wage_sum_tm4','wage_high_tm4','wage_sum_tm1t4']\n",
    "sel_features_demo=['age','marstat','homeless','hh_counts',\n",
    "'t_q1','t_q2','t_q3','edu_hs','edu_hsgrad','edu_coll','edu_collgrad','work_prof','work_othermgr','work_clerical',\n",
    "'work_sales','work_crafts','work_oper','work_service','work_labor','dist_cookcty','dist_downstate','race_1',\n",
    "'race_2','male','female','hh_white','hh_black','hh_native','hh_hispanic','hh_asian','foreignbn_1','foreignbn_2',\n",
    "'foreignbn_3','foreignbn_4','foreignbn_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional metrics that are often used are **precision** and **recall**. \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds we can vary and tune the precision and recall of a given classifier. A conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###precision and recall at k%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### magic loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_clfs_params(grid_size):\n",
    "    clfs={'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "       'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'AB':AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm='SAMME',n_estimators=200),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SVM':svm.SVC(kernel='linear',probability='true',random_state=0),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6,n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT':DecisionTreeClassifier(),\n",
    "        'SGD':SGDClassifier(loss='hinge',penalty='l2'),\n",
    "        'KNN':KNeighborsClassifier(n_neighbors=3)\n",
    "        }\n",
    "    small_grid={\n",
    "        'RF':{'n_estimators':[100,10000],'max_depth':[5,50],'max_features':['sqrt','log2'],'min_samples_split':[2,10]},\n",
    "        #'LR':{'penalty':['l1'],'C':[0.1]},\n",
    "        'LR':{'penalty':['l1','l2'],'C':[0.00001,0.001,0.1,1,10]},\n",
    "        'SGD':{'loss':['hinge','log','perceptron'],'penalty':['l2','l1','elasticnet']},\n",
    "        'ET':{'n_estimators':[10,100],'criterion':['gini','entropy'],'max_depth':[5,50],'max_features':['sqrt','log2'],\n",
    "             'min_samples_split':[2,10]},\n",
    "        'AB':{'algorithm':['SAMME','SAMME.R'],'n_estimators':[1,10,100,1000,10000]},\n",
    "        'GB':{'n_estimators':[10,100],'learning_rate':[0.001,0.1,0.5],'subsample':[0.1,0.5,1.0],'max_depth':[5,50]},\n",
    "        'NB':{},\n",
    "        'DT':{'criterion':['entropy'],'max_depth':[1,10,50],\n",
    "              'min_samples_split':[2,5,10]},\n",
    "        'SVM':{'C':[0.00001,0.0001,0.001,0.01,0.1,1,10],'kernel':['linear']},\n",
    "        'KNN':{'n_neighbors':[1,5,10,25,50,100],'weights':['uniform','distance'],\n",
    "               'algorithm':['auto','ball_tree','kd_tree']}\n",
    "    }\n",
    "    if (grid_size=='small'):\n",
    "        return clfs,small_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joint_sort_descending(l1,l2):\n",
    "    #l1,l2 have to be numpy arrays\n",
    "    idx=np.argsort(l1)[::-1]\n",
    "    return l1[idx],l2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_binary_at_k(y_scores,k):\n",
    "    cutoff_index=int(len(y_scores)*(k/100.0))\n",
    "    test_predictions_binary=[1 if x<cutoff_index else 0 for x in range(len(y_scores))]\n",
    "    return test_predictions_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    y_scores,y_true=joint_sort_descending(np.array(y_scores),np.array(y_true))\n",
    "    preds_at_k=generate_binary_at_k(y_scores,k)\n",
    "    precision=precision_score(y_true,preds_at_k)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "year = 2011\n",
    "year_minus_one=year-1\n",
    "print(year)\n",
    "print(year_minus_one)\n",
    "df_training=df[(df['oldspell_end']>=datetime.date(2010,5,31)) & (df['oldspell_end'] <=\n",
    "                                                              datetime.date(year_minus_one,12,31))].copy()\n",
    "df_testing=df[(df['oldspell_end']>=datetime.date(2010,5,31)) & (df['oldspell_end'] <=\n",
    "                                                              datetime.date(year,12,31))].copy()\n",
    "        #training_sql=\"\"\"SELECT * FROM c6.partial_evaluate WHERE (oldSpell_end>='2010-05-31'\n",
    "        #AND oldSpell_end<='{year_minus_one}-12-31')\"\"\".format(year_minus_one=year_minus_one)\n",
    "        #testing_sql=\"\"\"SELECT *  FROM c6.partial_evaluate WHERE (oldSpell_end>='{year}-01-01' \n",
    "        #AND oldSpell_end<='{year}-12-31')\"\"\".format(year=year)\n",
    "print(df_training.shape)\n",
    "print(df_testing.shape)\n",
    "print('baseline: Number of rows: {}'.format(df_training.shape[0]))\n",
    "print(df_training['return_1yr'].value_counts(normalize=True))\n",
    "        ##give pd dataframe a name\n",
    "        #trainset = pd.read_sql(training_sql, engine)\n",
    "        #testset = pd.read_sql(testing_sql, engine)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_training['age'].fillna(df_training['age'].mean(), inplace = True) \n",
    "df_testing['age'].fillna(df_testing['age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        df_training.fillna(df_training.mean(),inplace=True)\n",
    "        df_testing.fillna(df_testing.mean(),inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf_loop(models_to_run,clfs,grid):\n",
    "    results_df=pd.DataFrame(columns=('model_type','clf','parameters','auc-roc','p_at_5','p_at_10','p_at_10'))\n",
    "    for year in range(2011,2012):\n",
    "        \n",
    "        #create training and test set\n",
    "        year_minus_one=year-1\n",
    "        print(year)\n",
    "        print(year_minus_one)\n",
    "        df_training=df[(df['oldspell_end']>=datetime.date(2010,5,31)) & (df['oldspell_end'] <=\n",
    "                                                              datetime.date(year_minus_one,12,31))].copy()\n",
    "        df_testing=df[(df['oldspell_end']>=datetime.date(2010,5,31)) & (df['oldspell_end'] <=\n",
    "                                                              datetime.date(year,12,31))].copy()\n",
    "        #training_sql=\"\"\"SELECT * FROM c6.partial_evaluate WHERE (oldSpell_end>='2010-05-31'\n",
    "        #AND oldSpell_end<='{year_minus_one}-12-31')\"\"\".format(year_minus_one=year_minus_one)\n",
    "        #testing_sql=\"\"\"SELECT *  FROM c6.partial_evaluate WHERE (oldSpell_end>='{year}-01-01' \n",
    "        #AND oldSpell_end<='{year}-12-31')\"\"\".format(year=year)\n",
    "        print(df_training.shape)\n",
    "        print(df_testing.shape)\n",
    "       \n",
    "        ##give pd dataframe a name\n",
    "        #trainset = pd.read_sql(training_sql, engine)\n",
    "        #testset = pd.read_sql(testing_sql, engine)\n",
    "        #df_training.fillna(df_training.mean(),inplace=True)\n",
    "        #df_testing.fillna(df_testing.mean(),inplace=True)\n",
    "        print('Imputed with mean')\n",
    "        df_training['age'].fillna(df_training['age'].mean(), inplace = True) \n",
    "        df_testing['age'].fillna(df_testing['age'].mean(), inplace=True)\n",
    "        \n",
    "        \n",
    "        print(\"create np.array\") \n",
    "        y_train=df_training[sel_label].values\n",
    "        X_train=df_training[sel_features].values\n",
    "        y_test=df_testing[sel_label].values\n",
    "        X_test=df_testing[sel_features].values\n",
    "        \n",
    "        print('Scaling training and testing sets')\n",
    "        \n",
    "        #scaler=Normalizer().fit(X_train)\n",
    "        #normalized_X_train=scaler.transform(X_train)\n",
    "        #normalized_X_test=scaler.transform(X_test)\n",
    "        #print (\"std of un-normalized features for X_train:{}\".format(np.std(X_train)))\n",
    "        #print (\"std of un-normalized features for X_test:{}\".format(np.std(X_test)))\n",
    "        #print (\"std of normalized features for X_train:{}\".format(np.std(normalized_X_train)))\n",
    "        #print (\"std of normalized features for X_test:{}\".format(np.std(normalized_X_test)))\n",
    "        #X_train=normalized_X_train\n",
    "        #X_test=normalized_X_test\n",
    "        scaler=StandardScaler().fit(X_train)\n",
    "        scalerStandardScaler(with_mean=False)\n",
    "        scaled_X_train=scaler.transform(X_train)\n",
    "        scaled_X_test=scaler.transform(X_test)\n",
    "        print (\"mean of un-scaled features for X_train:{}\".format(np.mean(X_train)))\n",
    "        print (\"mean of un-scaled features for X_test:{}\".format(np.mean(X_test)))\n",
    "        print (\"mean of normalized features for X_train:{}\".format(np.mean(scaled_X_train)))\n",
    "        print (\"mean of normalized features for X_test:{}\".format(np.mean(scaled_X_test)))\n",
    "        X_train=scaled_X_train\n",
    "        X_test=scaled_X_test\n",
    "        print('baseline: Number of rows: {}'.format(df_training.shape[0]))\n",
    "        print(df_training['return_1yr'].value_counts(normalize=True))\n",
    "        \n",
    "        for index,clf in enumerate([clfs[x] for x in models_to_run]):\n",
    "            print models_to_run[index]\n",
    "            parameter_values=grid[models_to_run[index]]\n",
    "            for p in ParameterGrid(parameter_values):\n",
    "                try:\n",
    "                    clf.set_params(**p)\n",
    "                    y_pred_probs=clf.fit(X_train,y_train).predict_proba(X_test)[:,1]\n",
    "                    #you can also store the model, feature importances, and prediction scores\n",
    "                    #we're only string the metrics for now\n",
    "                    y_pred_probs_sorted,y_test_sorted=zip(*sorted(zip(y_pred_probs,y_test),reverse=True))\n",
    "                    results_df.loc[len(results_df)]=[models_to_run[index],clf,p,\n",
    "                                                     roc_auc_score(y_test,y_pred_probs),\n",
    "                                                     precision_at_k(y_test_sorted,y_pred_probs_sorted,5.0),\n",
    "                                                     precision_at_k(y_test_sorted,y_pred_probs_sorted,10.0),\n",
    "                                                     precision_at_k(y_test_sorted,y_pred_probs_sorted,20.0)]\n",
    "                    if NOTEBOOK==1:\n",
    "                        plot_precision_recall_n(y_test,y_pred_probs,clf)\n",
    "                except IndexError,e:\n",
    "                    print 'Error:',e\n",
    "                    continue\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    grid_size='small'\n",
    "    clfs,grid=define_clfs_params(grid_size)\n",
    "    models_to_run=['LR','RF','DT']\n",
    "    #'RF','DT','KNN','ET','AB','GB','NB'\n",
    "    #df=pd.read_csv(\"user/nnnnn.\")\n",
    "    #features=['','']\n",
    "    #X=df[features]\n",
    "    #y=df.return_1yr\n",
    "    results_df=clf_loop(models_to_run,clfs,grid)\n",
    "    if NOTEBOOK==1:\n",
    "        results_df\n",
    "    results_df.to_csv('results.csv',index=True)\n",
    "    \n",
    "    #master_results_df = pd.read_csv('results.csv')\n",
    "    #master_results_df = master_results_df.append(results_df)\n",
    "    #master_results_df.to_csv('results.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NOTEBOOK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#if _name_=='_main_':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
